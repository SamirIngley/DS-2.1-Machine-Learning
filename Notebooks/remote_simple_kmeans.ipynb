{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Images/slide_1_clustering.png\" width=\"700\" height=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "<img src=\"Images/slide_2_clustering.png\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Vectorization\n",
    "\n",
    "Question: What is text vectorization?\n",
    "\n",
    "Answer: **The process to transform text data to numerical vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need text vectorization?\n",
    "\n",
    "Think back to when we learned about **Label Encoding** and **One-Hot Encoding**: We took categories (text) and transformed them into numerical values. \n",
    "\n",
    "Text vectorization is similar in that we are taking text and turning it into something a machine can understand and manipulate by translating a word in to a unique vector of numbers. For example, we could associate the unique vector `(0, 1, 0, 1)` to the word `queen`.\n",
    "\n",
    "### Question: What are some other use cases for text vectorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use Cases for Text Vectorization\n",
    "\n",
    "- Count the number of unique words in each sentence (Bag-of-Words, we'll discuss this shortly!)\n",
    "\n",
    "- Assign weights to each word in the sentence.\n",
    "\n",
    "- Map each word to a number (dictionary with words as key and numbers as values) and represent each sentences as the sequence of numbers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag-of-Words Matrix\n",
    "\n",
    "- Bag-of-Words (BoW) is a matrix where its **rows are sentences** and its **columns are unique words** seen across all of the sentences\n",
    "\n",
    "### BoW Example\n",
    "\n",
    "We have the following 4 sentences:\n",
    "\n",
    "1. This is the first sentence.\n",
    "1. This one is the second sentence.\n",
    "1. And this is the third one.\n",
    "1. Is this the first sentence?\n",
    "\n",
    "**Question:** Given the above sentances, how many unique words are there?\n",
    "\n",
    "<!-- Answer: 9 -->\n",
    "\n",
    "A BoW matrix would look like the following, where `0` means the word does not appear in the sentence, and `1` means the word does appear in the sentence\n",
    "\n",
    "![bow_matrix](./Images/bag-of-words-matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW Worksheet (7 min)\n",
    "\n",
    "**Complete the following worksheet on your own:**\n",
    "\n",
    "- Copy [this blank table](https://docs.google.com/presentation/d/1B7v33fPEwblhHYBCSrCvKRBZz776Df4T_t2jcPXt4k8/edit?usp=sharing), and create the BoW matrix for the following sentences:\n",
    "\n",
    "\n",
    "1. Data Science is the best.\n",
    "1. Data Science has cool topics.\n",
    "1. Are these the best topics?\n",
    "1. Is Data Science the best track?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW in Sklearn\n",
    "\n",
    "- We can write a function to return a BoW matrix \n",
    "\n",
    "- Below, we will see how we can build a BoW matrix by calling [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn-feature-extraction-text-countvectorizer) in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "sentences = ['This is the first sentence.',\n",
    "             'This one is the second sentence.',\n",
    "             'And this is the third one.',\n",
    "             'Is this the first sentence?']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 1 0 1]\n",
      " [0 0 1 1 1 1 1 0 1]\n",
      " [1 0 1 1 0 0 1 1 1]\n",
      " [0 1 1 0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "# create a term-document matrix: assign each word a tuple: \n",
    "# first number is the sentence, and the second is the unique number that corresponds to the word\n",
    "# for example, if the word \"one\" is assigned the number 3,\n",
    "# then the word \"one\" that is used in the third sentence is represented by the tuple (2,3)\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# from the term-document matrix, create the BoW matrix\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we get unique words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'first', 'is', 'one', 'second', 'sentence', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Worksheet --> sklearn (7 min)\n",
    "\n",
    "Use sklearn to take the 4 sentences you used in the worksheet and create the BoW matrix using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "- Clustering is an unsupervised learning method. A **cluster** is a group of data points that are grouped together due to similarities in their features\n",
    "\n",
    "- This is very often used **because we usually donâ€™t have labeled data**\n",
    "\n",
    "- **K-Means clustering** is a popular clustering algorithms: it finds a fixed number _(k)_ of clusters in a set of data. \n",
    "\n",
    "- The goal of any cluster algorithm is to **find groups (clusters) in the given data**\n",
    "\n",
    "### Question: What are some use cases of clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of Clustering\n",
    "\n",
    "- Cluster movie dataset -> We expect the movies which their genres are similar be clustered in the same group\n",
    "\n",
    "- News Article Clustering -> We want the News related to science be in the same group, News related to sport be in the same group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo of K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'figures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-165cb4419f12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfigures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_kmeans_interactive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_kmeans_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'figures'"
     ]
    }
   ],
   "source": [
    "from figures import plot_kmeans_interactive\n",
    "\n",
    "plot_kmeans_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-means algorithm:\n",
    "\n",
    "Assume the inputs are $s_1$, $s_2$, ..., $s_n$. \n",
    "\n",
    "1. Choose a nnumber $K$ arbitrarily.\n",
    "1. Pick $K$ random points as cluster centers (called centroids)\n",
    "1. Assign each $s_i$ to nearest cluster by calculating its distance to each centroid\n",
    "1. Find a new cluster center by taking the average of the assigned points from step 3\n",
    "1. Repeat Step 3 and 4 until none of the cluster assignments change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's generate a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a sample dataset with 300 data points and 4 cluster centers\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                          random_state=0, cluster_std=0.60)\n",
    "\n",
    "# plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# k-means algorithm where k = 4\n",
    "km = KMeans(n_clusters=4)\n",
    "# perform k-means clustering on the previous dataset\n",
    "km.fit(X)\n",
    "# print the 4 cluster centers\n",
    "print(km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-min Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to choose the optimal number (K) of clusters?\n",
    "\n",
    "We could always choose a high number, but we may be wasting a lot of time and resources when a smaller number would give us the same results. How do we know the best `K` to pick so that we are running k-means as efficiently as possible?\n",
    "\n",
    "### The Elbow Method\n",
    "\n",
    "We can find the optimal `K` by utilizing the **Elbow Method:** a method that assigns a score to each `K`. When we plot these scores, we will get a line that looks like an arm bending at the elbow. **The `K` value that is closest to the \"elbow\" point of the graph is our optimal `K`**\n",
    "\n",
    "Scores can be calculated two different ways:\n",
    "\n",
    "1. **Distortion:** the average of the squared distances from each sample to its closest cluster center. Typically, the Euclidean distance metric is used. The lower the distortion, the better the score\n",
    "    1. For numberes 1 to `k`, compute the following:\n",
    "        1. Euclidean squared distance formula: $\\sum_{j=1}^{k} (a_j-b_j)^2$\n",
    "        1. For each sample, find the squared distance between the sample and all `k` cluster centers, and then pick the closest center (shortest distance)\n",
    "        1. Take the average of the above\n",
    "1. **Inertia:** the sum of squared distances of samples to their closest cluster center. The lower the inertia, the better the score\n",
    "    1. We'll use the same Euclidean squared distance formula for here as well.\n",
    "\n",
    "Either scoring method is valid, and will give you the same optimal `K` value. Below we will look at how to implement both scoring methods:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "distortions = []\n",
    "K = range(1, 10)\n",
    "for k in K:\n",
    "    # fit the k-means for a given k to the data (X)\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X)\n",
    "    # distance.cdist finds the squared distances\n",
    "    # axis=1 allows us to keep the min for each sample, not jsut the min across the entire dataset\n",
    "    # find the closest distance for each sample to a center, and take the average\n",
    "    distortions.append(sum(np.min(distance.cdist(X, km.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow: bx- = use a solid (-) blue (b) line, \n",
    "# and mark the x-axis points with an x (x)\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X)\n",
    "    # inertia is an attribute of km!\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "    sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "\n",
    "    # Plot the elbow\n",
    "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity - Elbow Method (7 min)\n",
    "\n",
    "Using the starter code below, prove that 6 is the optimal `K` for clustering the data using k-means using the elbow method. You can use either Distortion or Inertia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Data, blob_y = make_blobs(n_samples=500, centers=6,\n",
    "                          random_state=0, cluster_std=0.80)\n",
    "\n",
    "# plot the data\n",
    "plt.scatter(Data[:, 0], Data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity: Combine Text Vectorization and Clustering the Texts (30 min)\n",
    "\n",
    "**Complete the activity below in groups of 3**\n",
    "\n",
    "- We want to cluster the given sentences \n",
    "\n",
    "- To do this: We to use both concepts we learned today:\n",
    "\n",
    "    - Vectorize the sentences (text-vectorization)\n",
    "    \n",
    "    - Apply Kmeans to cluster our vectorized sentences\n",
    "\n",
    "- **Note:** We want to remove stop words from our sentences (and, or, is, etc.). To do this, we add `stop_words='english'` to our call to `CountVectorize`\n",
    "\n",
    "- **Hint:** Look at the sentences in the starter code. How would you cluster the data if you were doing the clustering? Use that number as your `K` to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "sentences = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "sentences = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "# remove stop words from sentences (and, or, is, ...) and instantiate the Bag-of-Word \n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# transform sentences into numerical arrays \n",
    "X = vectorizer.fit_transform(sentences)\n",
    "# print unique words (vocabulary)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n",
    "# We know there are two group of sentences -> Group 1: cats | Group 2: Google\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++')\n",
    "model.fit(X)\n",
    "\n",
    "# Testing our model: For a new sentence, let's see how the model will cluster it. \n",
    "\n",
    "# first we should convert the sentence to a numerical array\n",
    "Y = vectorizer.transform([\"chrome browser to open.\"])\n",
    "print('Y:')\n",
    "print(Y.toarray())\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "# Let's do the same for another sentence\n",
    "Y = vectorizer.transform([\"My cat is hungry.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "# Lets see the model prediction for training docs\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other clustering methods and comparison:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources:\n",
    "\n",
    "- https://www.youtube.com/watch?v=FrmrHyOSyhE\n",
    "\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- In order to work with text, we should transform sentences into vectors of numbers\n",
    "\n",
    "- We learned a method for text vectorization -> Bag-of-Words (CountVectorizer)\n",
    "\n",
    "    - We will learn TFIDF Vectorizer next session\n",
    "    \n",
    "\n",
    "- Clustering is an unsupervised learning algorithm that obtains groups based on the geometric positions of features\n",
    "\n",
    "- K-means is one clustering method that separates the data into `K` number of clusters. The Elbow method can be used to find the optimal `K`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Obtain the centers (centriods) of two cluster: which words would be close to the centriods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', 'app', 'belly', 'best', 'came', 'cat', 'chrome', 'climbing', 'eating', 'extension', 'face', 'feedback', 'google', 'impressed', 'incredible', 'key', 'kitten', 'kitty', 'little', 'map', 'merley', 'ninja', 'open', 'photo', 'play', 'promoter', 'restaurant', 'smiley', 'squooshy', 'tab', 'taken', 'translate', 've']\n",
      "(8, 33)\n",
      "Y:\n",
      "[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]]\n",
      "[0]\n",
      "[1]\n",
      "[1 1 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "sentences = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++')\n",
    "model.fit(X)\n",
    "# print('M:')\n",
    "# print(model.cluster_centers_.argsort())\n",
    "# print(model.cluster_centers_.argsort()[:, ::-1])\n",
    "# print(\"Top terms per cluster:\")\n",
    "# order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "# terms = vectorizer.get_feature_names()\n",
    "# for i in range(true_k):\n",
    "#     print(\"Cluster %d:\" % i),\n",
    "#     for ind in order_centroids[i, :10]:\n",
    "#         print(' %s' % terms[ind]),\n",
    "\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"chrome browser to open.\"])\n",
    "print('Y:')\n",
    "print(Y.toarray())\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"My cat is hungry.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "# Lets see the model prediction for training docs\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
